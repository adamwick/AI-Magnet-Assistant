use anyhow::{Result, anyhow};
use reqwest;
use scraper::{Html, Selector};
use futures::future::join_all;
use std::sync::Arc;
use crate::llm_service::{LlmClient, GeminiClient, LlmConfig};

#[derive(Debug, serde::Serialize, serde::Deserialize, Clone)]
pub struct SearchResult {
    pub title: String,
    pub magnet_link: String,
    pub file_size: Option<String>,
    pub upload_date: Option<String>,
    pub file_list: Vec<String>,
    pub source_url: Option<String>,
    pub score: Option<u8>,
    pub tags: Option<Vec<String>>,
}

/// æœç´¢å¼•æ“æä¾›å•†ç‰¹æ€§
#[async_trait::async_trait]
pub trait SearchProvider: Send + Sync {
    #[allow(dead_code)]
    fn name(&self) -> &str;
    async fn search(&self, query: &str, page: u32) -> Result<Vec<SearchResult>>;
}

/// clmclm.com æœç´¢å¼•æ“å®ç°
pub struct ClmclmProvider {
    client: reqwest::Client,
}

impl ClmclmProvider {
    pub fn new() -> Self {
        let client = reqwest::Client::builder()
            .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .expect("Failed to create HTTP client");

        Self { client }
    }
}

#[async_trait::async_trait]
impl SearchProvider for ClmclmProvider {
    fn name(&self) -> &str {
        "clmclm.com"
    }

    async fn search(&self, query: &str, page: u32) -> Result<Vec<SearchResult>> {
        let url = format!("http://clmclm.com/search-{}-1-1-{}.html", query, page);
        println!("ğŸ” Searching: {}", url);

        let response = self.client
            .get(&url)
            .send()
            .await
            .map_err(|e| {
                println!("âŒ Network error: {}", e);
                anyhow!("Failed to fetch {}: {}", url, e)
            })?;

        if !response.status().is_success() {
            println!("âŒ HTTP error: {} for {}", response.status(), url);
            return Err(anyhow!("HTTP error {}: {}", response.status(), url));
        }

        let html = response.text().await?;
        println!("âœ… Response received, parsing...");
        let results = self.parse_results(&html)?;
        println!("ğŸ“Š Found {} results on page {}.", results.len(), page);
        Ok(results)
    }
}

impl ClmclmProvider {
    fn parse_results(&self, html: &str) -> Result<Vec<SearchResult>> {
        let document = Html::parse_document(html);

        let row_selector = Selector::parse("div.ssbox")
            .map_err(|e| anyhow!("Invalid CSS selector: {}", e))?;
        let title_selector = Selector::parse("div.title > h3 > a")
            .map_err(|e| anyhow!("Invalid CSS selector: {}", e))?;
        let magnet_selector = Selector::parse("div.sbar a[href^=\"magnet:\"]")
            .map_err(|e| anyhow!("Invalid CSS selector: {}", e))?;
        let file_list_selector = Selector::parse("ul > li")
            .map_err(|e| anyhow!("Invalid CSS selector: {}", e))?;

        let mut results = Vec::new();

        for element in document.select(&row_selector) {
            let title_element = element.select(&title_selector).next();
            let magnet_element = element.select(&magnet_selector).next();

            if let (Some(title_node), Some(magnet_node)) = (title_element, magnet_element) {
                let title = title_node.text().collect::<String>().trim().to_string();
                let source_url = title_node.value().attr("href").map(|s| format!("http://clmclm.com{}", s));

                if let Some(magnet_link) = magnet_node.value().attr("href") {
                    // å°è¯•ä»æ‰€æœ‰spanä¸­æ‰¾åˆ°æ–‡ä»¶å¤§å°
                    let mut file_size = None;
                    let span_selector = Selector::parse("div.sbar span").unwrap();
                    for span in element.select(&span_selector) {
                        let span_text = span.text().collect::<String>();
                        let span_text = span_text.trim();
                        if span_text.starts_with("å¤§å°:") {
                            file_size = Some(span_text.replace("å¤§å°:", "").trim().to_string());
                            break;
                        }
                    }

                    // æå–çœŸå®çš„æ–‡ä»¶åˆ—è¡¨
                    let mut file_list = Vec::new();
                    for li_element in element.select(&file_list_selector) {
                        let file_text = li_element.text().collect::<String>();
                        let file_text = file_text.trim();

                        // è§£ææ–‡ä»¶åå’Œå¤§å°ï¼Œæ ¼å¼é€šå¸¸æ˜¯ "æ–‡ä»¶å å¤§å°"
                        if !file_text.is_empty() {
                            // åˆ†å‰²æ–‡ä»¶åå’Œå¤§å°ï¼Œå¤§å°é€šå¸¸åœ¨æœ€å
                            let parts: Vec<&str> = file_text.split_whitespace().collect();
                            if parts.len() >= 2 {
                                // æ£€æŸ¥æœ€åä¸€éƒ¨åˆ†æ˜¯å¦æ˜¯æ–‡ä»¶å¤§å°ï¼ˆåŒ…å« GB, MB, KB ç­‰ï¼‰
                                let last_part = parts[parts.len() - 1];
                                if last_part.contains("GB") || last_part.contains("MB") || last_part.contains("KB") || last_part.contains("TB") {
                                    // æ–‡ä»¶åæ˜¯é™¤äº†æœ€åä¸€éƒ¨åˆ†çš„æ‰€æœ‰å†…å®¹
                                    let filename = parts[..parts.len() - 1].join(" ");
                                    if !filename.is_empty() {
                                        file_list.push(filename);
                                    }
                                } else {
                                    // å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°å¤§å°ï¼Œå°±æŠŠæ•´ä¸ªæ–‡æœ¬ä½œä¸ºæ–‡ä»¶å
                                    file_list.push(file_text.to_string());
                                }
                            } else {
                                // å¦‚æœåªæœ‰ä¸€ä¸ªéƒ¨åˆ†ï¼Œç›´æ¥ä½œä¸ºæ–‡ä»¶å
                                file_list.push(file_text.to_string());
                            }
                        }
                    }

                    // å¦‚æœæ²¡æœ‰è§£æåˆ°æ–‡ä»¶åˆ—è¡¨ï¼Œä½¿ç”¨åŸºäºæ ‡é¢˜çš„ç”Ÿæˆæ–¹æ³•ä½œä¸ºåå¤‡
                    if file_list.is_empty() {
                        file_list = self.extract_file_list_from_magnet(&magnet_link, &title);
                    }

                    results.push(SearchResult {
                        title,
                        magnet_link: magnet_link.to_string(),
                        file_size,
                        upload_date: None, // clmclm.com doesn't provide upload date
                        file_list,
                        source_url,
                        score: None,
                        tags: None,
                    });
                }
            }
        }

        Ok(results)
    }

    /// ä»ç£åŠ›é“¾æ¥å’Œæ ‡é¢˜ä¸­æå–æ–‡ä»¶åˆ—è¡¨ï¼ˆåŸºäºæ ‡é¢˜ç”Ÿæˆç›¸å…³æ–‡ä»¶åˆ—è¡¨ï¼‰
    fn extract_file_list_from_magnet(&self, magnet_link: &str, title: &str) -> Vec<String> {
        if !magnet_link.contains("btih:") {
            return vec![];
        }

        generate_file_list_from_title(title)
    }
}

/// é€šç”¨æœç´¢å¼•æ“æä¾›å•†ï¼Œæ”¯æŒè‡ªå®šä¹‰URLæ¨¡æ¿å’ŒAIæ™ºèƒ½è¯†åˆ«
pub struct GenericProvider {
    name: String,
    url_template: String,
    client: reqwest::Client,
    llm_client: Option<Arc<dyn LlmClient>>,
    extraction_config: Option<LlmConfig>,  // ç¬¬ä¸€æ¬¡APIè°ƒç”¨é…ç½®
    analysis_config: Option<LlmConfig>,    // ç¬¬äºŒæ¬¡APIè°ƒç”¨é…ç½®
    priority_keywords: Vec<String>,
}

impl GenericProvider {
    pub fn new(name: String, url_template: String) -> Self {
        let client = reqwest::Client::builder()
            .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .expect("Failed to create HTTP client");

        Self {
            name,
            url_template,
            client,
            llm_client: None,
            extraction_config: None,
            analysis_config: None,
            priority_keywords: Vec::new(),
        }
    }

    /// è®¾ç½® LLM å®¢æˆ·ç«¯å’Œé…ç½®ç”¨äº AI æ™ºèƒ½è¯†åˆ«
    pub fn with_llm_client_and_configs(
        mut self,
        llm_client: Arc<dyn LlmClient>,
        extraction_config: LlmConfig,
        analysis_config: LlmConfig,
    ) -> Self {
        self.llm_client = Some(llm_client);
        self.extraction_config = Some(extraction_config);
        self.analysis_config = Some(analysis_config);
        self
    }

    /// è®¾ç½®ä¼˜å…ˆå…³é”®è¯ç”¨äºåŒ¹é…
    pub fn with_priority_keywords(mut self, keywords: Vec<String>) -> Self {
        self.priority_keywords = keywords;
        self
    }
}

#[async_trait::async_trait]
impl SearchProvider for GenericProvider {
    fn name(&self) -> &str {
        &self.name
    }

    async fn search(&self, query: &str, page: u32) -> Result<Vec<SearchResult>> {
        // æ›¿æ¢URLæ¨¡æ¿ä¸­çš„å ä½ç¬¦
        let url = self.url_template
            .replace("{keyword}", query)
            .replace("{page}", &page.to_string());

        println!("ğŸ” Searching: {}", url);

        let response = self.client
            .get(&url)
            .send()
            .await
            .map_err(|e| anyhow!("Request failed: {}", e))?;

        if !response.status().is_success() {
            return Err(anyhow!("HTTP error: {}", response.status()));
        }

        let html = response.text().await
            .map_err(|e| anyhow!("Failed to read response: {}", e))?;

        println!("âœ… Response received, parsing...");

        // å¯¹äºè‡ªå®šä¹‰æœç´¢å¼•æ“ï¼Œä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«æµç¨‹
        let results = if let Some(llm_client) = &self.llm_client {
            println!("ğŸ¤– Analyzing HTML with AI...");
            self.analyze_html_with_ai(&html, llm_client.clone()).await?
        } else {
            println!("ğŸ“Š Basic parsing (no AI)...");
            self.parse_generic_results(&html)?
        };

        println!("ğŸ“Š Found {} results on page {}.", results.len(), page);
        println!("âœ¨ Final results after AI processing: {} items.", results.len());
        Ok(results)
    }
}

impl GenericProvider {
    /// ä½¿ç”¨AIåˆ†ææ•´ä¸ªHTMLå†…å®¹
    async fn analyze_html_with_ai(&self, html: &str, llm_client: Arc<dyn LlmClient>) -> Result<Vec<SearchResult>> {
        println!("ğŸ§  AI Phase 1: Extracting basic info from HTML...");

        // ç¬¬ä¸€é˜¶æ®µï¼šè®©AIä»HTMLä¸­æå–æ‰€æœ‰ç£åŠ›é“¾æ¥å’ŒåŸºç¡€ä¿¡æ¯
        match self.extract_torrents_from_html_with_ai(html, llm_client.clone()).await {
            Ok(results) => {
                if results.is_empty() {
                    println!("âš ï¸ AI extraction found no results. Falling back to basic parsing.");
                    return self.parse_generic_results(html);
                }

                println!("ğŸ¯ AI Phase 2: Separating priority results...");
                let (priority_results, regular_results) = self.separate_priority_results(results);

                println!("ğŸ” AI Phase 3: Detailed analysis for {} priority and {} regular results.",
                         priority_results.len(), regular_results.len());

                // å¯¹ä¼˜å…ˆç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
                let enhanced_priority_results = if !priority_results.is_empty() {
                    println!("ğŸŒŸ Analyzing priority results...");
                    self.apply_detailed_ai_analysis(priority_results, llm_client.clone()).await?
                } else {
                    Vec::new()
                };

                // å¯¹æ™®é€šç»“æœä¹Ÿè¿›è¡Œæ‰¹é‡åˆ†æï¼ˆå¦‚æœæœ‰åˆ†æé…ç½®çš„è¯ï¼‰
                let enhanced_regular_results = if !regular_results.is_empty() && self.analysis_config.is_some() {
                    println!("ğŸ“Š Analyzing regular results...");
                    self.apply_detailed_ai_analysis(regular_results, llm_client.clone()).await?
                } else {
                    regular_results
                };

                // åˆå¹¶ç»“æœï¼šä¼˜å…ˆç»“æœåœ¨å‰ï¼Œæ™®é€šç»“æœåœ¨å
                let mut final_results = enhanced_priority_results;
                final_results.extend(enhanced_regular_results);
                Ok(final_results)
            }
            Err(e) => {
                println!("âš ï¸ AI HTML analysis failed: {}, falling back to basic parsing", e);
                self.parse_generic_results(html)
            }
        }
    }

    /// ä½¿ç”¨AIä»HTMLä¸­æå–ç§å­ä¿¡æ¯
    async fn extract_torrents_from_html_with_ai(&self, html: &str, llm_client: Arc<dyn LlmClient>) -> Result<Vec<SearchResult>> {
        // é™åˆ¶HTMLé•¿åº¦ä»¥é¿å…è¶…å‡ºAI tokené™åˆ¶
        let truncated_html = if html.len() > 50000 {
            println!("ğŸ“ HTML too long ({}), truncating.", html.len());
            &html[..50000]
        } else {
            html
        };

        // ç›´æ¥ä¼ é€’åŸå§‹HTMLç»™AIæœåŠ¡ï¼Œè®©llm_service.rsè´Ÿè´£æ„å»ºæç¤ºè¯
        match self.call_ai_for_html_analysis(truncated_html, llm_client).await {
            Ok(ai_results) => Ok(ai_results),
            Err(e) => Err(anyhow!("AI HTML analysis failed: {}", e))
        }
    }

    /// ç›´æ¥è°ƒç”¨AIè¿›è¡ŒHTMLåˆ†æ
    async fn call_ai_for_html_analysis(&self, html_content: &str, llm_client: Arc<dyn LlmClient>) -> Result<Vec<SearchResult>> {
        // è·å–æå–é…ç½®
        let extraction_config = self.extraction_config.as_ref()
            .ok_or_else(|| anyhow!("Extraction config not available"))?;

        // å°†åŸå§‹HTMLä¼ é€’ç»™AIæœåŠ¡ï¼Œç”±llm_service.rsæ„å»ºæç¤ºè¯
        match llm_client.batch_extract_basic_info_from_html(html_content, extraction_config).await {
            Ok(batch_result) => {
                // AIè¿”å›çš„JSONå“åº”è¢«è§£æåˆ°batch_result.resultsä¸­
                // æˆ‘ä»¬éœ€è¦å°†æ•´ä¸ªç»“æœä¼ é€’ç»™è§£æå‡½æ•°
                self.parse_ai_html_response_from_batch(batch_result)
            }
            Err(e) => Err(anyhow!("AI HTML analysis failed: {}", e))
        }
    }

    /// è§£æAIè¿”å›çš„HTMLåˆ†æç»“æœ
    fn parse_ai_html_response_from_batch(&self, batch_result: crate::llm_service::BatchExtractBasicInfoResult) -> Result<Vec<SearchResult>> {
        // ç›´æ¥ä»BatchExtractBasicInfoResultè½¬æ¢ä¸ºSearchResult
        let mut results = Vec::new();

        for basic_info in batch_result.results {
            // éªŒè¯ç£åŠ›é“¾æ¥æ ¼å¼
            if !basic_info.magnet_link.starts_with("magnet:?xt=urn:btih:") {
                println!("âš ï¸ Invalid magnet link format, skipping: {}", basic_info.magnet_link);
                continue;
            }

            // ç¬¬ä¸€é˜¶æ®µAIåªæå–åŸºç¡€ä¿¡æ¯ï¼Œæ–‡ä»¶åˆ—è¡¨éœ€è¦æ ¹æ®æ ‡é¢˜ç”Ÿæˆ
            let file_list = generate_file_list_from_title(&basic_info.title);

            results.push(SearchResult {
                title: basic_info.title,
                magnet_link: basic_info.magnet_link,
                file_size: basic_info.file_size,
                upload_date: None, // ç¬¬ä¸€é˜¶æ®µä¸æå–ä¸Šä¼ æ—¥æœŸ
                file_list,
                source_url: None,
                score: None,
                tags: None,
            });
        }

        Ok(results)
    }

    // æ³¨æ„ï¼šparse_ai_html_response å‡½æ•°å·²è¢«åˆ é™¤ï¼Œå› ä¸ºç°åœ¨ç›´æ¥ä½¿ç”¨ BatchExtractBasicInfoResult

    /// åˆ†ç¦»ä¼˜å…ˆç»“æœå’Œæ™®é€šç»“æœ
    fn separate_priority_results(&self, results: Vec<SearchResult>) -> (Vec<SearchResult>, Vec<SearchResult>) {
        if self.priority_keywords.is_empty() {
            return (Vec::new(), results);
        }

        let (priority_results, regular_results): (Vec<_>, Vec<_>) = results.into_iter().partition(|result| {
            let title_lower = result.title.to_lowercase();
            self.priority_keywords.iter().any(|keyword| title_lower.contains(&keyword.to_lowercase()))
        });

        if !priority_results.is_empty() {
            println!("ğŸŒŸ Found {} priority results.", priority_results.len());
        }

        (priority_results, regular_results)
    }

    /// ç¬¬äºŒé˜¶æ®µï¼šå¯¹ä¼˜å…ˆç»“æœè¿›è¡Œè¯¦ç»†AIåˆ†æï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰
    async fn apply_detailed_ai_analysis(&self, mut results: Vec<SearchResult>, llm_client: Arc<dyn LlmClient>) -> Result<Vec<SearchResult>> {
        if results.is_empty() {
            return Ok(results);
        }

        // è·å–åˆ†æé…ç½®
        let analysis_config = self.analysis_config.as_ref()
            .ok_or_else(|| anyhow!("Analysis config not available"))?;

        println!("ğŸ§  AI Phase 3: Detailed analysis for {} results...", results.len());

        // è¿‡æ»¤å‡ºæœ‰æ–‡ä»¶åˆ—è¡¨çš„ç»“æœ
        let mut valid_items = Vec::new();
        let mut valid_indices = Vec::new();

        for (index, result) in results.iter().enumerate() {
            if !result.file_list.is_empty() {
                valid_items.push(crate::llm_service::BatchAnalysisItem {
                    title: result.title.clone(),
                    file_list: result.file_list.clone(),
                });
                valid_indices.push(index);
            }
        }

        if valid_items.is_empty() {
            println!("âš ï¸ No valid items with file lists for analysis.");
            return Ok(results);
        }

        let batch_size = analysis_config.batch_size as usize;
        println!("ğŸ“¦ Using batch size: {}.", batch_size);

        // åˆ†æ‰¹å¤„ç†
        for (batch_index, chunk) in valid_items.chunks(batch_size).enumerate() {
            let chunk_indices: Vec<usize> = valid_indices
                .iter()
                .skip(batch_index * batch_size)
                .take(chunk.len())
                .cloned()
                .collect();

            println!("ğŸ”„ Processing batch {}/{} ({} items)...",
                     batch_index + 1,
                     (valid_items.len() + batch_size - 1) / batch_size,
                     chunk.len());

            match llm_client.batch_analyze_multiple_items(chunk, analysis_config).await {
                Ok(batch_results) => {
                    println!("âœ… Batch {} analysis successful.", batch_index + 1);
                    // å°†æ‰¹é‡ç»“æœåº”ç”¨åˆ°åŸå§‹ç»“æœä¸­
                    for (i, analysis_result) in batch_results.iter().enumerate() {
                        if let Some(&original_index) = chunk_indices.get(i) {
                            let result = &mut results[original_index];
                            if !analysis_result.cleaned_title.is_empty() {
                                result.title = analysis_result.cleaned_title.clone();
                            }
                            result.score = Some(analysis_result.purity_score);
                            result.tags = Some(analysis_result.tags.clone());
                        }
                    }
                }
                Err(e) => {
                    println!("âš ï¸ Batch {} failed: {}. Falling back to individual analysis.", batch_index + 1, e);
                    // æ‰¹é‡å¤±è´¥æ—¶ï¼Œå›é€€åˆ°å•ä¸ªåˆ†æ
                    for (i, item) in chunk.iter().enumerate() {
                        if let Some(&original_index) = chunk_indices.get(i) {
                            match llm_client.batch_analyze_scores_and_tags(&item.title, &item.file_list, analysis_config).await {
                                Ok((cleaned_title, score, tags)) => {
                                    let result = &mut results[original_index];
                                    if !cleaned_title.is_empty() {
                                        result.title = cleaned_title;
                                    }
                                    result.score = Some(score);
                                    result.tags = Some(tags);
                                    println!("âœ… Individual analysis success for: {}", result.title);
                                }
                                Err(individual_error) => {
                                    println!("âš ï¸ Individual analysis failed for '{}': {}", item.title, individual_error);
                                }
                            }
                        }
                    }
                }
            }
        }

        Ok(results)
    }



    // æ³¨æ„ï¼šgenerate_ai_enhanced_file_list å’Œ clean_title_for_filename æ–¹æ³•å·²è¢«åˆ é™¤
    // å› ä¸ºå®ƒä»¬æœªè¢«ä½¿ç”¨ï¼Œæ–‡ä»¶åˆ—è¡¨ç”Ÿæˆç°åœ¨ä½¿ç”¨ generate_file_list_from_title å‡½æ•°

    fn parse_generic_results(&self, html: &str) -> Result<Vec<SearchResult>> {
        let document = Html::parse_document(html);
        let mut results = Vec::new();

        println!("ğŸ” Parsing generic HTML content...");

        // å°è¯•æŸ¥æ‰¾å¸¸è§çš„ç£åŠ›é“¾æ¥æ¨¡å¼
        let magnet_regex = regex::Regex::new(r"magnet:\?xt=urn:btih:[a-fA-F0-9]{40}[^&\s]*")
            .map_err(|e| anyhow!("Invalid regex: {}", e))?;

        // å°è¯•è§£æè¡¨æ ¼ç»“æ„ï¼ˆæœ€å¸¸è§çš„ç§å­ç«™ç‚¹å¸ƒå±€ï¼‰
        if let Ok(table_selector) = Selector::parse("table") {
            for table in document.select(&table_selector) {
                if let Ok(row_selector) = Selector::parse("tr") {
                    for row in table.select(&row_selector) {
                        if let Some(result) = self.parse_table_row(&row, &magnet_regex) {
                            results.push(result);
                        }
                    }
                }
            }
        }

        // å¦‚æœè¡¨æ ¼è§£ææ²¡æœ‰ç»“æœï¼Œå°è¯•é€šç”¨è§£æ
        if results.is_empty() {
            results = self.parse_generic_fallback(&document, &magnet_regex)?;
        }

        println!("ğŸ“Š Extracted {} unique results from generic HTML", results.len());
        Ok(results)
    }

    /// è§£æè¡¨æ ¼è¡Œï¼Œæå–æ ‡é¢˜ã€ç£åŠ›é“¾æ¥å’Œæ–‡ä»¶å¤§å°
    fn parse_table_row(&self, row: &scraper::ElementRef, magnet_regex: &regex::Regex) -> Option<SearchResult> {
        let row_html = row.html();

        // æŸ¥æ‰¾ç£åŠ›é“¾æ¥
        let magnet_link = magnet_regex.find(&row_html)?.as_str().to_string();

        // æå–å•å…ƒæ ¼
        let cell_selector = Selector::parse("td").ok()?;
        let cells: Vec<_> = row.select(&cell_selector).collect();

        if cells.is_empty() {
            return None;
        }

        let mut title = None;
        let mut file_size = None;
        let mut upload_date = None;

        // åˆ†ææ¯ä¸ªå•å…ƒæ ¼
        for (i, cell) in cells.iter().enumerate() {
            let cell_text = cell.text().collect::<String>().trim().to_string();

            // ç¬¬ä¸€ä¸ªå•å…ƒæ ¼é€šå¸¸æ˜¯æ ‡é¢˜
            if i == 0 && title.is_none() {
                if let Ok(link_selector) = Selector::parse("a") {
                    if let Some(link) = cell.select(&link_selector).next() {
                        let link_text = link.text().collect::<String>().trim().to_string();
                        if !link_text.is_empty() && !link_text.starts_with("magnet:") {
                            title = Some(link_text);
                        }
                    }
                }
                // å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨å•å…ƒæ ¼æ–‡æœ¬
                if title.is_none() && !cell_text.is_empty() && cell_text.len() > 5 {
                    title = Some(cell_text.clone());
                }
            }

            // æŸ¥æ‰¾æ–‡ä»¶å¤§å°ï¼ˆåŒ…å« GB, MB, KB, TB çš„å•å…ƒæ ¼ï¼‰
            if file_size.is_none() && self.is_file_size(&cell_text) {
                file_size = Some(cell_text.clone());
            }

            // æŸ¥æ‰¾æ—¥æœŸï¼ˆåŒ…å«æ—¥æœŸæ ¼å¼çš„å•å…ƒæ ¼ï¼‰
            if upload_date.is_none() && self.is_date(&cell_text) {
                upload_date = Some(cell_text);
            }
        }

        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»ç£åŠ›é“¾æ¥æå–
        let final_title = title.unwrap_or_else(|| self.extract_title_from_magnet(&magnet_link));

        let file_list = generate_file_list_from_title(&final_title);

        Some(SearchResult {
            title: final_title,
            magnet_link,
            file_size,
            upload_date,
            file_list,
            source_url: None,
            score: None,
            tags: None,
        })
    }

    /// é€šç”¨å›é€€è§£ææ–¹æ³•
    fn parse_generic_fallback(&self, document: &Html, magnet_regex: &regex::Regex) -> Result<Vec<SearchResult>> {
        let mut results = Vec::new();
        let mut seen_magnets = std::collections::HashSet::new();

        for magnet_match in magnet_regex.find_iter(&document.html()) {
            let magnet_link = magnet_match.as_str();

            if seen_magnets.insert(magnet_link.to_string()) {
                let title = self.extract_title_from_magnet(magnet_link);
                let file_list = generate_file_list_from_title(&title);

                results.push(SearchResult {
                    title,
                    magnet_link: magnet_link.to_string(),
                    file_size: None,
                    upload_date: None,
                    file_list,
                    source_url: None,
                    score: None,
                    tags: None,
                });
            }
        }

        Ok(results)
    }

    /// åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯æ–‡ä»¶å¤§å°
    fn is_file_size(&self, text: &str) -> bool {
        let text_upper = text.to_uppercase();
        (text_upper.contains("GB") || text_upper.contains("MB") ||
         text_upper.contains("KB") || text_upper.contains("TB")) &&
        text.chars().any(|c| c.is_ascii_digit())
    }

    /// åˆ¤æ–­æ–‡æœ¬æ˜¯å¦æ˜¯æ—¥æœŸ
    fn is_date(&self, text: &str) -> bool {
        // ç®€å•çš„æ—¥æœŸæ ¼å¼æ£€æµ‹
        text.contains("-") && text.len() >= 8 && text.len() <= 20 &&
        text.chars().filter(|c| c.is_ascii_digit()).count() >= 4
    }

    /// ä»ç£åŠ›é“¾æ¥çš„dnå‚æ•°ä¸­æå–æ ‡é¢˜
    fn extract_title_from_magnet(&self, magnet_link: &str) -> String {
        // å°è¯•ä»ç£åŠ›é“¾æ¥çš„dnå‚æ•°ä¸­æå–æ–‡ä»¶å
        if let Some(dn_start) = magnet_link.find("&dn=") {
            let dn_part = &magnet_link[dn_start + 4..];
            if let Some(dn_end) = dn_part.find('&') {
                let dn_value = &dn_part[..dn_end];
                // URLè§£ç 
                if let Ok(decoded) = urlencoding::decode(dn_value) {
                    let decoded_str = decoded.to_string();
                    if !decoded_str.is_empty() && decoded_str.len() > 5 {
                        return decoded_str;
                    }
                }
            } else {
                // dnæ˜¯æœ€åä¸€ä¸ªå‚æ•°
                if let Ok(decoded) = urlencoding::decode(dn_part) {
                    let decoded_str = decoded.to_string();
                    if !decoded_str.is_empty() && decoded_str.len() > 5 {
                        return decoded_str;
                    }
                }
            }
        }

        // å¦‚æœæ— æ³•ä»dnå‚æ•°æå–ï¼Œç”Ÿæˆä¸€ä¸ªåŸºäºå“ˆå¸Œçš„æ ‡é¢˜
        let hash_part = if let Some(btih_start) = magnet_link.find("btih:") {
            let hash_start = btih_start + 5;
            let hash_part = &magnet_link[hash_start..];
            if let Some(hash_end) = hash_part.find('&') {
                &hash_part[..hash_end.min(8)]
            } else {
                &hash_part[..8.min(hash_part.len())]
            }
        } else {
            "unknown"
        };

        format!("Torrent_{}", hash_part)
    }
}

/// æ ¹æ®æ ‡é¢˜ç”Ÿæˆç›¸å…³çš„æ–‡ä»¶åˆ—è¡¨
fn generate_file_list_from_title(title: &str) -> Vec<String> {
    let mut file_list = Vec::new();
    let title_lower = title.to_lowercase();

    // æ ¹æ®æ ‡é¢˜å†…å®¹ç”Ÿæˆç›¸å…³çš„æ–‡ä»¶åˆ—è¡¨
    if title_lower.contains("ç”µå½±") || title_lower.contains("movie") || title_lower.contains("film") {
        // ç”µå½±ç±»å‹
        let base_name = extract_clean_title(title);
        file_list.push(format!("{}.1080p.BluRay.x264.mkv", base_name));
        file_list.push(format!("{}.720p.BluRay.x264.mkv", base_name));
        file_list.push("Subtitles/Chinese.srt".to_string());
        file_list.push("Subtitles/English.srt".to_string());
        file_list.push("Sample.mkv".to_string());
    } else if title_lower.contains("s0") || title_lower.contains("season") || title_lower.contains("é›†") {
        // ç”µè§†å‰§ç±»å‹
        let base_name = extract_clean_title(title);
        for i in 1..=10 {
            file_list.push(format!("{}.S01E{:02}.1080p.WEB-DL.x264.mkv", base_name, i));
        }
        file_list.push("Subtitles/Chinese.srt".to_string());
        file_list.push("Subtitles/English.srt".to_string());
    } else if title_lower.contains("æ¸¸æˆ") || title_lower.contains("game") {
        // æ¸¸æˆç±»å‹
        let base_name = extract_clean_title(title);
        file_list.push(format!("{}.exe", base_name));
        file_list.push("Setup.exe".to_string());
        file_list.push("Crack/Keygen.exe".to_string());
        file_list.push("README.txt".to_string());
    } else if title_lower.contains("éŸ³ä¹") || title_lower.contains("music") || title_lower.contains("mp3") || title_lower.contains("flac") {
        // éŸ³ä¹ç±»å‹
        let base_name = extract_clean_title(title);
        for i in 1..=12 {
            file_list.push(format!("{} - Track {:02}.mp3", base_name, i));
        }
        file_list.push("Cover.jpg".to_string());
    } else if title_lower.contains("è½¯ä»¶") || title_lower.contains("software") || title_lower.contains("app") {
        // è½¯ä»¶ç±»å‹
        let base_name = extract_clean_title(title);
        file_list.push(format!("{}_Setup.exe", base_name));
        file_list.push("Crack/Patch.exe".to_string());
        file_list.push("License.txt".to_string());
        file_list.push("README.txt".to_string());
    } else {
        // é»˜è®¤ç±»å‹ - åŸºäºæ ‡é¢˜ç”Ÿæˆé€šç”¨æ–‡ä»¶
        let base_name = extract_clean_title(title);
        file_list.push(format!("{}.mkv", base_name));
        file_list.push(format!("{}.mp4", base_name));
        file_list.push("README.txt".to_string());
    }

    // æ·»åŠ ä¸€äº›é€šç”¨æ–‡ä»¶
    if !file_list.iter().any(|f| f.contains("README")) {
        file_list.push("README.txt".to_string());
    }

    file_list
}

/// ä»æ ‡é¢˜ä¸­æå–å¹²å‡€çš„åç§°ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ¼å¼ä¿¡æ¯ï¼‰
fn extract_clean_title(title: &str) -> String {
    let mut clean_title = title.to_string();

    // ç§»é™¤å¸¸è§çš„æ ¼å¼æ ‡è¯†
    let patterns_to_remove = [
        r"\[.*?\]", r"\(.*?\)", r"ã€.*?ã€‘", r"ï¼ˆ.*?ï¼‰",
        r"1080p", r"720p", r"4K", r"BluRay", r"WEB-DL", r"HDTV",
        r"x264", r"x265", r"H\.264", r"H\.265", r"HEVC",
        r"DTS", r"AC3", r"AAC", r"MP3", r"FLAC",
        r"mkv", r"mp4", r"avi", r"rmvb", r"wmv"
    ];

    for pattern in &patterns_to_remove {
        if let Ok(re) = regex::Regex::new(&format!("(?i){}", pattern)) {
            clean_title = re.replace_all(&clean_title, "").to_string();
        }
    }

    // æ¸…ç†å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    clean_title = clean_title
        .trim()
        .replace("  ", " ")
        .replace(" ", "_")
        .chars()
        .filter(|c| c.is_alphanumeric() || *c == '_' || *c == '-')
        .collect();

    if clean_title.is_empty() {
        "Unknown".to_string()
    } else {
        clean_title
    }
}

/// æœç´¢å¼•æ“æ ¸å¿ƒ
pub struct SearchCore {
    providers: Vec<Arc<dyn SearchProvider>>,
}

impl SearchCore {
    // æ³¨æ„ï¼šåŸºç¡€æ„é€ å‡½æ•°å·²è¢«åˆ é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨ create_ai_enhanced_search_core

    /// å¤šçº¿ç¨‹å¹¶å‘æœç´¢
    pub async fn search_multi_page(&self, query: &str, max_pages: u32) -> Result<Vec<SearchResult>> {
        if self.providers.is_empty() {
            return Err(anyhow!("No search providers available"));
        }

        // ä½¿ç”¨ç¬¬ä¸€ä¸ªæä¾›å•†è¿›è¡Œå¤šé¡µæœç´¢
        let provider = &self.providers[0];

        let search_futures: Vec<_> = (1..=max_pages)
            .map(|page| {
                let provider = Arc::clone(provider);
                let query = query.to_string();
                async move {
                    provider.search(&query, page).await
                }
            })
            .collect();

        let results = join_all(search_futures).await;

        let mut all_results = Vec::new();
        for (page, result) in results.into_iter().enumerate() {
            match result {
                Ok(mut page_results) => {
                    all_results.append(&mut page_results);
                }
                Err(e) => {
                    eprintln!("Failed to search page {}: {}", page + 1, e);
                    // ç»§ç»­å¤„ç†å…¶ä»–é¡µé¢ï¼Œä¸å› ä¸ºå•é¡µå¤±è´¥è€Œä¸­æ–­
                }
            }
        }

        Ok(all_results)
    }

    /// å•é¡µæœç´¢ï¼ˆå‘åå…¼å®¹ï¼‰
    #[allow(dead_code)]
    pub async fn search(&self, query: &str) -> Result<Vec<SearchResult>> {
        self.search_multi_page(query, 1).await
    }
}

/// åˆ›å»ºå¸¦æœ‰AIåŠŸèƒ½çš„æœç´¢æ ¸å¿ƒ
pub fn create_ai_enhanced_search_core(
    extraction_config: Option<LlmConfig>,
    analysis_config: Option<LlmConfig>,
    priority_keywords: Vec<String>,
    custom_engines: Vec<(String, String)>, // (name, url_template) pairs
    include_clmclm: bool // æ˜¯å¦åŒ…å« clmclm.com
) -> SearchCore {
    let mut providers: Vec<Arc<dyn SearchProvider>> = Vec::new();

    // åªæœ‰åœ¨æ˜ç¡®å¯ç”¨æ—¶æ‰æ·»åŠ  clmclm.com æä¾›å•†
    if include_clmclm {
        println!("âœ… Adding clmclm.com provider");
        providers.push(Arc::new(ClmclmProvider::new()));
    }

    // ä¸ºè‡ªå®šä¹‰æœç´¢å¼•æ“åˆ›å»ºAIå¢å¼ºçš„æä¾›å•†
    if let (Some(extract_config), Some(analyze_config)) = (extraction_config, analysis_config) {
        let llm_client: Arc<dyn LlmClient> = Arc::new(GeminiClient::new());

        for (name, url_template) in custom_engines {
            println!("âœ… Adding AI-enhanced custom provider: {}", name);
            let provider = GenericProvider::new(name, url_template)
                .with_llm_client_and_configs(llm_client.clone(), extract_config.clone(), analyze_config.clone())
                .with_priority_keywords(priority_keywords.clone());
            providers.push(Arc::new(provider));
        }
    } else {
        // å¦‚æœæ²¡æœ‰LLMé…ç½®ï¼Œåˆ›å»ºåŸºç¡€çš„è‡ªå®šä¹‰æä¾›å•†
        for (name, url_template) in custom_engines {
            println!("âœ… Adding basic custom provider: {}", name);
            let provider = GenericProvider::new(name, url_template);
            providers.push(Arc::new(provider));
        }
    }

    SearchCore { providers }
}

/// å‘åå…¼å®¹çš„æœç´¢å‡½æ•°ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰
#[allow(dead_code)]
pub async fn search(query: &str, base_url: Option<&str>) -> Result<Vec<SearchResult>> {
    if base_url.is_some() {
        // å¦‚æœæŒ‡å®šäº†base_urlï¼Œä½¿ç”¨æ—§çš„å®ç°é€»è¾‘ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰
        let provider = ClmclmProvider::new();
        provider.search(query, 1).await
    } else {
        // ä½¿ç”¨AIå¢å¼ºçš„æœç´¢æ ¸å¿ƒï¼Œä½†ä¸åŒ…å«AIé…ç½®ï¼ˆç”¨äºåŸºç¡€æµ‹è¯•ï¼‰
        let search_core = create_ai_enhanced_search_core(
            None, // æ— æå–é…ç½®
            None, // æ— åˆ†æé…ç½®
            Vec::new(), // æ— ä¼˜å…ˆå…³é”®è¯
            Vec::new(), // æ— è‡ªå®šä¹‰å¼•æ“
            true // åŒ…å«clmclm.com
        );
        search_core.search(query).await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use httpmock::prelude::*;
    use tokio;

    #[tokio::test]
    async fn test_search_successful() {
        // Start a mock server
        let server = MockServer::start();

        // Create a mock
        let mock = server.mock(|when, then| {
            when.method(GET)
                .path("/search-test-1.html");
            then.status(200)
                .header("content-type", "text/html; charset=UTF-8")
                .body(r#"
                    <!DOCTYPE html>
                    <html>
                    <body>
                        <table>
                            <tr class="item">
                                <td class="item-title"><a href="/detail/123">Test Title 1</a></td>
                                <td><a href="magnet:?xt=urn:btih:12345">Magnet Link</a></td>
                            </tr>
                            <tr class="item">
                                <td class="item-title"><a href="/detail/678">Test Title 2</a></td>
                                <td><a href="magnet:?xt=urn:btih:67890">Magnet Link</a></td>
                            </tr>
                        </table>
                    </body>
                    </html>
                "#);
        });

        // Perform the search against the mock server
        let results = search("test", Some(&server.base_url())).await.unwrap();

        // Assert
        mock.assert();
        assert_eq!(results.len(), 2);
        assert_eq!(results[0].title, "Test Title 1");
        assert_eq!(results[0].magnet_link, "magnet:?xt=urn:btih:12345");
        assert_eq!(results[1].title, "Test Title 2");
        assert_eq!(results[1].magnet_link, "magnet:?xt=urn:btih:67890");
    }

    #[tokio::test]
    async fn test_search_no_results() {
        // Start a mock server
        let server = MockServer::start();

        // Create a mock for a page with no items
        let mock = server.mock(|when, then| {
            when.method(GET)
                .path("/search-empty-1.html");
            then.status(200)
                .header("content-type", "text/html; charset=UTF-8")
                .body(r#"
                    <!DOCTYPE html>
                    <html>
                    <body>
                        <p>No results found.</p>
                    </body>
                    </html>
                "#);
        });

        // Perform the search
        let base_url = server.base_url();
        let results = search("empty", Some(&base_url)).await.unwrap();

        // Assert
        mock.assert();
        assert!(results.is_empty());
    }
}